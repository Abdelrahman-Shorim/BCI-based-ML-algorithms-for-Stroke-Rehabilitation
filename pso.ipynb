{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "if os.path.exists('data.pkl'):\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        X, y = pickle.load(f)\n",
    "else:\n",
    "    X= tf.convert_to_tensor(X_reshaped, dtype=tf.float32)\n",
    "    y=tf.convert_to_tensor(y_categorical, dtype=tf.float32)\n",
    "    with open('data.pkl', 'wb') as f:\n",
    "        pickle.dump((X, y), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.8 * X.shape[0])  \n",
    "X_train, X_test = tf.split(X, [split_index, X.shape[0] - split_index])\n",
    "y_train, y_test = tf.split(y, [split_index, y.shape[0] - split_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Iteration 2/100\n",
      "Iteration 3/100\n",
      "Iteration 4/100\n",
      "Iteration 5/100\n",
      "Iteration 6/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The `kernel_size` argument must be a tuple of 2 integers. Received: (5, 0) including {0} that does not satisfy the requirement `> 0`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msplit(X, [split_index, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m split_index])\n\u001b[0;32m     88\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msplit(y, [split_index, y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m split_index])\n\u001b[1;32m---> 90\u001b[0m best_individual, best_fitness \u001b[38;5;241m=\u001b[39m \u001b[43mpso\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest CNN Architecture found:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_individual)\n",
      "Cell \u001b[1;32mIn[4], line 54\u001b[0m, in \u001b[0;36mpso\u001b[1;34m(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, individual \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(population):\n\u001b[1;32m---> 54\u001b[0m     fitness \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_cnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fitness \u001b[38;5;241m>\u001b[39m evaluate_cnn_model(personal_best[i], input_shape, num_classes, X_train, y_train, X_test, y_test):\n\u001b[0;32m     56\u001b[0m         personal_best[i] \u001b[38;5;241m=\u001b[39m individual\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mevaluate_cnn_model\u001b[1;34m(params, input_shape, num_classes, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_conv_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 23\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_filters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilter_sizes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39moutput_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39moutput_shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     25\u001b[0m         model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m \u001b[43minit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layout_param_name, layout \u001b[38;5;129;01min\u001b[39;00m layout_args\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\conv2d.py:179\u001b[0m, in \u001b[0;36mConv2D.__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mallow_initializer_layout\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    178\u001b[0m ):\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_initializer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_regularizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_regularizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivity_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivity_regularizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_constraint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_constraint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:136\u001b[0m, in \u001b[0;36mConv.__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, conv_op, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m=\u001b[39m filters\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups \u001b[38;5;241m=\u001b[39m groups \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m=\u001b[39m \u001b[43mconv_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_tuple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkernel_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides \u001b[38;5;241m=\u001b[39m conv_utils\u001b[38;5;241m.\u001b[39mnormalize_tuple(\n\u001b[0;32m    140\u001b[0m     strides, rank, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;241m=\u001b[39m conv_utils\u001b[38;5;241m.\u001b[39mnormalize_padding(padding)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\conv_utils.py:113\u001b[0m, in \u001b[0;36mnormalize_tuple\u001b[1;34m(value, n, name, allow_zero)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unqualified_values:\n\u001b[0;32m    109\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m including \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munqualified_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m that does not satisfy the requirement `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreq_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value_tuple\n",
      "\u001b[1;31mValueError\u001b[0m: The `kernel_size` argument must be a tuple of 2 integers. Received: (5, 0) including {0} that does not satisfy the requirement `> 0`."
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "search_space = {\n",
    "    'num_conv_layers': range(1, 4),\n",
    "    'num_filters': [16, 32, 64],\n",
    "    'filter_sizes': [(3, 3), (5, 5)],\n",
    "    'num_dense_layers': range(1, 4),\n",
    "    'dense_layer_units': [32, 64, 128]\n",
    "}\n",
    "\n",
    "population_size = 10\n",
    "num_iterations = 100\n",
    "c1 = 2.0\n",
    "c2 = 2.0\n",
    "w = 0.7\n",
    "convergence = []\n",
    "def evaluate_cnn_model(params, input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    for _ in range(params['num_conv_layers']):\n",
    "        model.add(layers.Conv2D(params['num_filters'], params['filter_sizes'], activation='relu', padding='same'))\n",
    "        if model.output_shape[1] > 1 and model.output_shape[2] > 1:\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "        else:\n",
    "            model.add(layers.MaxPooling2D((1, 1)))\n",
    "    model.add(layers.Flatten())\n",
    "    for _ in range(params['num_dense_layers']):\n",
    "        model.add(layers.Dense(params['dense_layer_units'], activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x=X_train, y=y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def initialize_population(population_size, search_space):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        individual = {}\n",
    "        for param, values in search_space.items():\n",
    "            if param == 'filter_sizes':\n",
    "                filter_size = random.choice(values)\n",
    "                while 0 in filter_size:\n",
    "                    filter_size = random.choice(values)\n",
    "                individual[param] = filter_size\n",
    "            else:\n",
    "                individual[param] = random.choice(values)\n",
    "        population.append(individual)\n",
    "    return population\n",
    "\n",
    "\n",
    "def pso(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    population = initialize_population(population_size, search_space)\n",
    "    personal_best = population.copy()\n",
    "    global_best = None\n",
    "    global_best_fitness = -1\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
    "        for i, individual in enumerate(population):\n",
    "            fitness = evaluate_cnn_model(individual, input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "            if fitness > evaluate_cnn_model(personal_best[i], input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "                personal_best[i] = individual.copy()\n",
    "            if fitness > global_best_fitness:\n",
    "                global_best = individual.copy()\n",
    "                global_best_fitness = fitness\n",
    "\n",
    "        convergence.append(global_best_fitness)\n",
    "\n",
    "        velocity = [{param: [0, 0] if param == 'filter_sizes' else 0 for param in search_space.keys()} for _ in range(population_size)]\n",
    "\n",
    "        for i, individual in enumerate(population):\n",
    "            for param, value in search_space.items():\n",
    "                if param == 'filter_sizes':\n",
    "                    velocity[i][param] = (\n",
    "                        w * velocity[i][param][0] + c1 * random.random() * (personal_best[i][param][0] - individual[param][0]) + c2 * random.random() * (global_best[param][0] - individual[param][0]),\n",
    "                        w * velocity[i][param][1] + c1 * random.random() * (personal_best[i][param][1] - individual[param][1]) + c2 * random.random() * (global_best[param][1] - individual[param][1])\n",
    "                    )\n",
    "                    individual[param] = (\n",
    "                        int(round(individual[param][0] + velocity[i][param][0])),\n",
    "                        int(round(individual[param][1] + velocity[i][param][1]))\n",
    "                    )\n",
    "                else:\n",
    "                    velocity[i][param] = w * velocity[i][param] + c1 * random.random() * (personal_best[i][param] - individual[param]) + c2 * random.random() * (global_best[param] - individual[param])\n",
    "                    individual[param] = int(round(individual[param] + velocity[i][param]))\n",
    "                    individual[param] = max(min(individual[param], max(value)), min(value))\n",
    "\n",
    "    return global_best, global_best_fitness\n",
    "\n",
    "\n",
    "input_shape = (22, 4, 1)\n",
    "num_classes = 4\n",
    "split_index = int(0.8 * X.shape[0])\n",
    "X_train, X_test = tf.split(X, [split_index, X.shape[0] - split_index])\n",
    "y_train, y_test = tf.split(y, [split_index, y.shape[0] - split_index])\n",
    "\n",
    "best_individual, best_fitness = pso(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "print(\"Best CNN Architecture found:\")\n",
    "print(best_individual)\n",
    "print(\"Best Accuracy:\", \"%.3f\" %best_fitness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, ELU, Dropout, Dense, Flatten\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_classifier(input_shape, num_classes, num_filters_conv1, num_filters_conv2):\n",
    "    classifier = Sequential()\n",
    "\n",
    "    classifier.add(Conv2D(num_filters_conv1, 32, input_shape=input_shape, padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(0.01)))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(ELU())\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    for itr in range(1):\n",
    "        classifier.add(Conv2D(num_filters_conv2, 8, padding='same',\n",
    "                              kernel_regularizer=regularizers.l2(0.01)))\n",
    "        classifier.add(BatchNormalization())\n",
    "        classifier.add(ELU())\n",
    "        classifier.add(Conv2D(num_filters_conv2, 1, padding='valid'))\n",
    "        classifier.add(BatchNormalization())\n",
    "        classifier.add(ELU())\n",
    "        classifier.add(Dropout(0.5))\n",
    "        classifier.add(Flatten())\n",
    "        classifier.add(Dense(units=num_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def evaluate_eeg_naet_model(num_filters_conv1, num_filters_conv2):\n",
    "    classifier = build_classifier(input_shape=(22, 4, 1), num_classes=4,\n",
    "                                  num_filters_conv1=int(num_filters_conv1),\n",
    "                                  num_filters_conv2=int(num_filters_conv2))\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = classifier.fit(x=X_train, y=y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    _, accuracy = classifier.evaluate(X_test, y_test)\n",
    "    return -accuracy\n",
    "\n",
    "def pso_optimization_eeg_naet(bounds, num_particles, num_iterations):\n",
    "    particles = np.random.uniform(bounds[0], bounds[1], (num_particles, 2))\n",
    "    personal_best_positions = particles.copy()\n",
    "    personal_best_scores = np.array([evaluate_eeg_naet_model(p[0], p[1]) for p in particles])\n",
    "    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "    global_best_score = np.min(personal_best_scores)\n",
    "\n",
    "    convergence = []  # Initialize an array to track convergence\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        for i, particle in enumerate(particles):\n",
    "            velocity = np.random.uniform(-1, 1, 2)\n",
    "            new_position = particle + velocity\n",
    "            new_position = np.clip(new_position, bounds[0], bounds[1])\n",
    "            particles[i] = new_position\n",
    "\n",
    "            score = evaluate_eeg_naet_model(new_position[0], new_position[1])\n",
    "            if score < personal_best_scores[i]:\n",
    "                personal_best_positions[i] = new_position\n",
    "                personal_best_scores[i] = score\n",
    "\n",
    "                if score < global_best_score:\n",
    "                    global_best_position = new_position\n",
    "                    global_best_score = score\n",
    "\n",
    "        convergence.append(global_best_score)\n",
    "\n",
    "    return global_best_position, -global_best_score, convergence\n",
    "\n",
    "bounds = [(16, 64), (16, 64)]\n",
    "\n",
    "best_hyperparameters, best_accuracy, convergence = pso_optimization_eeg_naet(bounds, num_particles=10, num_iterations=100)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hyperparameters)\n",
    "print(\"Best accuracy found:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "def create_mlp_model(input_shape, num_classes, num_neurons_dense1, num_neurons_dense2):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=input_shape))\n",
    "    model.add(layers.Dense(num_neurons_dense1, activation='relu'))\n",
    "    model.add(layers.Dense(num_neurons_dense2, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  \n",
    "    return model\n",
    "\n",
    "def evaluate_mlp_model(num_neurons_dense1, num_neurons_dense2):\n",
    "    mlp_model = create_mlp_model(input_shape=(22, 4), num_classes=4,\n",
    "                                 num_neurons_dense1=int(num_neurons_dense1),\n",
    "                                 num_neurons_dense2=int(num_neurons_dense2))\n",
    "    mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = mlp_model.fit(x=X_train, y=y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    _, accuracy = mlp_model.evaluate(X_test, y_test)\n",
    "    return -accuracy\n",
    "\n",
    "def pso_optimization_mlp(bounds, num_particles, num_iterations):\n",
    "    particles = np.random.uniform(bounds[0], bounds[1], (num_particles, 2))\n",
    "    personal_best_positions = particles.copy()\n",
    "    personal_best_scores = np.array([evaluate_mlp_model(p[0], p[1]) for p in particles])\n",
    "    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "    global_best_score = np.min(personal_best_scores)\n",
    "\n",
    "    convergence = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        for i, particle in enumerate(particles):\n",
    "            velocity = np.random.uniform(-1, 1, 2)\n",
    "            new_position = particle + velocity\n",
    "            new_position = np.clip(new_position, bounds[0], bounds[1])\n",
    "            particles[i] = new_position\n",
    "\n",
    "            score = evaluate_mlp_model(new_position[0], new_position[1])\n",
    "            if score < personal_best_scores[i]:\n",
    "                personal_best_positions[i] = new_position\n",
    "                personal_best_scores[i] = score\n",
    "\n",
    "                if score < global_best_score:\n",
    "                    global_best_position = new_position\n",
    "                    global_best_score = score\n",
    "\n",
    "        convergence.append(global_best_score)\n",
    "\n",
    "    return global_best_position, -global_best_score, convergence\n",
    "\n",
    "bounds = [(32, 256), (16, 128)]\n",
    "\n",
    "best_hyperparameters, best_accuracy, convergence = pso_optimization_mlp(bounds, num_particles=10, num_iterations=100)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hyperparameters)\n",
    "print(\"Best accuracy found:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "search_space = {\n",
    "    'num_units': [32, 64, 128],\n",
    "    'num_layers': range(1, 4)\n",
    "}\n",
    "\n",
    "population_size = 10\n",
    "num_iterations = 100\n",
    "c1 = 2.0\n",
    "c2 = 2.0\n",
    "w = 0.7\n",
    "\n",
    "def create_rnn_model(params, input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.SimpleRNN(params['num_units'], activation='relu', input_shape=input_shape))\n",
    "\n",
    "    for _ in range(params['num_layers']):\n",
    "        model.add(layers.Dense(params['num_units'], activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_rnn_model(params, input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    model = create_rnn_model(params, input_shape, num_classes)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x=X_train, y=y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def pso(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    population = initialize_population(population_size, search_space)\n",
    "    velocity = [{param: 0 for param in search_space.keys()} for _ in range(population_size)]\n",
    "    personal_best = population.copy()\n",
    "    global_best = None\n",
    "    global_best_fitness = -1\n",
    "\n",
    "    convergence = []\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
    "        for i, individual in enumerate(population):\n",
    "            fitness = evaluate_rnn_model(individual, input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "            if fitness > evaluate_rnn_model(personal_best[i], input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "                personal_best[i] = individual.copy()\n",
    "            if fitness > global_best_fitness:\n",
    "                global_best = individual.copy()\n",
    "                global_best_fitness = fitness\n",
    "\n",
    "            for param in search_space.keys():\n",
    "                velocity[i][param] = w * velocity[i][param] + c1 * random.random() * (personal_best[i][param] - individual[param]) + c2 * random.random() * (global_best[param] - individual[param])\n",
    "                individual[param] = int(round(individual[param] + velocity[i][param]))\n",
    "                individual[param] = max(min(individual[param], max(search_space[param])), min(search_space[param]))\n",
    "\n",
    "        convergence.append(global_best_fitness)\n",
    "\n",
    "    return global_best, global_best_fitness, convergence\n",
    "\n",
    "def initialize_population(population_size, search_space):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        individual = {param: random.choice(values) for param, values in search_space.items()}\n",
    "        population.append(individual)\n",
    "    return population\n",
    "\n",
    "input_shape = (22, 4)\n",
    "num_classes = 4\n",
    "split_index = int(0.8 * X.shape[0])\n",
    "X_train, X_test = tf.split(X, [split_index, X.shape[0] - split_index])\n",
    "y_train, y_test = tf.split(y, [split_index, y.shape[0] - split_index])\n",
    "\n",
    "best_individual, best_fitness = pso(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "print(\"Best RNN Model Architecture found:\")\n",
    "print(best_individual)\n",
    "print(\"Best Accuracy:\", \"%.3f\" %best_fitness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
