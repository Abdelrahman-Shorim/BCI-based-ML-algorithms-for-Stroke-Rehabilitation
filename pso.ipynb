{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "if os.path.exists('data.pkl'):\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        X, y = pickle.load(f)\n",
    "else:\n",
    "    X= tf.convert_to_tensor(X_reshaped, dtype=tf.float32)\n",
    "    y=tf.convert_to_tensor(y_categorical, dtype=tf.float32)\n",
    "    with open('data.pkl', 'wb') as f:\n",
    "        pickle.dump((X, y), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.8 * X.shape[0])  \n",
    "X_train, X_test = tf.split(X, [split_index, X.shape[0] - split_index])\n",
    "y_train, y_test = tf.split(y, [split_index, y.shape[0] - split_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "search_space = {\n",
    "    'num_conv_layers': range(1, 4),\n",
    "    'num_filters': [16, 32, 64],\n",
    "    'filter_sizes': [(3, 3), (5, 5)],\n",
    "    'num_dense_layers': range(1, 4),\n",
    "    'dense_layer_units': [32, 64, 128]\n",
    "}\n",
    "\n",
    "population_size = 10\n",
    "num_iterations = 100\n",
    "c1 = 2.0\n",
    "c2 = 2.0\n",
    "w = 0.7\n",
    "convergence = []\n",
    "def evaluate_cnn_model(params, input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    for _ in range(params['num_conv_layers']):\n",
    "        model.add(layers.Conv2D(params['num_filters'], params['filter_sizes'], activation='relu', padding='same'))\n",
    "        if model.output_shape[1] > 1 and model.output_shape[2] > 1:\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "        else:\n",
    "            model.add(layers.MaxPooling2D((1, 1)))\n",
    "    model.add(layers.Flatten())\n",
    "    for _ in range(params['num_dense_layers']):\n",
    "        model.add(layers.Dense(params['dense_layer_units'], activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x=X_train, y=y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def initialize_population(population_size, search_space):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        individual = {}\n",
    "        for param, values in search_space.items():\n",
    "            if param == 'filter_sizes':\n",
    "                filter_size = random.choice(values)\n",
    "                while 0 in filter_size:\n",
    "                    filter_size = random.choice(values)\n",
    "                individual[param] = filter_size\n",
    "            else:\n",
    "                individual[param] = random.choice(values)\n",
    "        population.append(individual)\n",
    "    return population\n",
    "\n",
    "\n",
    "def pso(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    population = initialize_population(population_size, search_space)\n",
    "    personal_best = population.copy()\n",
    "    global_best = None\n",
    "    global_best_fitness = -1\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
    "        for i, individual in enumerate(population):\n",
    "            fitness = evaluate_cnn_model(individual, input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "            if fitness > evaluate_cnn_model(personal_best[i], input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "                personal_best[i] = individual.copy()\n",
    "            if fitness > global_best_fitness:\n",
    "                global_best = individual.copy()\n",
    "                global_best_fitness = fitness\n",
    "\n",
    "        convergence.append(global_best_fitness)\n",
    "\n",
    "        velocity = [{param: [0, 0] if param == 'filter_sizes' else 0 for param in search_space.keys()} for _ in range(population_size)]\n",
    "\n",
    "        for i, individual in enumerate(population):\n",
    "            for param, value in search_space.items():\n",
    "                if param == 'filter_sizes':\n",
    "                    velocity[i][param] = (\n",
    "                        w * velocity[i][param][0] + c1 * random.random() * (personal_best[i][param][0] - individual[param][0]) + c2 * random.random() * (global_best[param][0] - individual[param][0]),\n",
    "                        w * velocity[i][param][1] + c1 * random.random() * (personal_best[i][param][1] - individual[param][1]) + c2 * random.random() * (global_best[param][1] - individual[param][1])\n",
    "                    )\n",
    "                    individual[param] = (\n",
    "                        int(round(individual[param][0] + velocity[i][param][0])),\n",
    "                        int(round(individual[param][1] + velocity[i][param][1]))\n",
    "                    )\n",
    "                else:\n",
    "                    velocity[i][param] = w * velocity[i][param] + c1 * random.random() * (personal_best[i][param] - individual[param]) + c2 * random.random() * (global_best[param] - individual[param])\n",
    "                    individual[param] = int(round(individual[param] + velocity[i][param]))\n",
    "                    individual[param] = max(min(individual[param], max(value)), min(value))\n",
    "\n",
    "    return global_best, global_best_fitness\n",
    "\n",
    "\n",
    "input_shape = (22, 4, 1)\n",
    "num_classes = 4\n",
    "split_index = int(0.8 * X.shape[0])\n",
    "X_train, X_test = tf.split(X, [split_index, X.shape[0] - split_index])\n",
    "y_train, y_test = tf.split(y, [split_index, y.shape[0] - split_index])\n",
    "\n",
    "best_individual, best_fitness = pso(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "print(\"Best CNN Architecture found:\")\n",
    "print(best_individual)\n",
    "print(\"Best Accuracy:\", \"%.3f\" %best_fitness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, ELU, Dropout, Dense, Flatten\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_classifier(input_shape, num_classes, num_filters_conv1, num_filters_conv2):\n",
    "    classifier = Sequential()\n",
    "\n",
    "    classifier.add(Conv2D(num_filters_conv1, 32, input_shape=input_shape, padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(0.01)))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(ELU())\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    for itr in range(1):\n",
    "        classifier.add(Conv2D(num_filters_conv2, 8, padding='same',\n",
    "                              kernel_regularizer=regularizers.l2(0.01)))\n",
    "        classifier.add(BatchNormalization())\n",
    "        classifier.add(ELU())\n",
    "        classifier.add(Conv2D(num_filters_conv2, 1, padding='valid'))\n",
    "        classifier.add(BatchNormalization())\n",
    "        classifier.add(ELU())\n",
    "        classifier.add(Dropout(0.5))\n",
    "        classifier.add(Flatten())\n",
    "        classifier.add(Dense(units=num_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def evaluate_eeg_naet_model(num_filters_conv1, num_filters_conv2):\n",
    "    classifier = build_classifier(input_shape=(22, 4, 1), num_classes=4,\n",
    "                                  num_filters_conv1=int(num_filters_conv1),\n",
    "                                  num_filters_conv2=int(num_filters_conv2))\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = classifier.fit(x=X_train, y=y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    _, accuracy = classifier.evaluate(X_test, y_test)\n",
    "    return -accuracy\n",
    "\n",
    "def pso_optimization_eeg_naet(bounds, num_particles, num_iterations):\n",
    "    particles = np.random.uniform(bounds[0], bounds[1], (num_particles, 2))\n",
    "    personal_best_positions = particles.copy()\n",
    "    personal_best_scores = np.array([evaluate_eeg_naet_model(p[0], p[1]) for p in particles])\n",
    "    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "    global_best_score = np.min(personal_best_scores)\n",
    "\n",
    "    convergence = []  # Initialize an array to track convergence\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        for i, particle in enumerate(particles):\n",
    "            velocity = np.random.uniform(-1, 1, 2)\n",
    "            new_position = particle + velocity\n",
    "            new_position = np.clip(new_position, bounds[0], bounds[1])\n",
    "            particles[i] = new_position\n",
    "\n",
    "            score = evaluate_eeg_naet_model(new_position[0], new_position[1])\n",
    "            if score < personal_best_scores[i]:\n",
    "                personal_best_positions[i] = new_position\n",
    "                personal_best_scores[i] = score\n",
    "\n",
    "                if score < global_best_score:\n",
    "                    global_best_position = new_position\n",
    "                    global_best_score = score\n",
    "\n",
    "        convergence.append(global_best_score)\n",
    "\n",
    "    return global_best_position, -global_best_score, convergence\n",
    "\n",
    "bounds = [(16, 64), (16, 64)]\n",
    "\n",
    "best_hyperparameters, best_accuracy, convergence = pso_optimization_eeg_naet(bounds, num_particles=10, num_iterations=100)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hyperparameters)\n",
    "print(\"Best accuracy found:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "def create_mlp_model(input_shape, num_classes, num_neurons_dense1, num_neurons_dense2):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=input_shape))\n",
    "    model.add(layers.Dense(num_neurons_dense1, activation='relu'))\n",
    "    model.add(layers.Dense(num_neurons_dense2, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  \n",
    "    return model\n",
    "\n",
    "def evaluate_mlp_model(num_neurons_dense1, num_neurons_dense2):\n",
    "    mlp_model = create_mlp_model(input_shape=(22, 4), num_classes=4,\n",
    "                                 num_neurons_dense1=int(num_neurons_dense1),\n",
    "                                 num_neurons_dense2=int(num_neurons_dense2))\n",
    "    mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = mlp_model.fit(x=X_train, y=y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    _, accuracy = mlp_model.evaluate(X_test, y_test)\n",
    "    return -accuracy\n",
    "\n",
    "def pso_optimization_mlp(bounds, num_particles, num_iterations):\n",
    "    particles = np.random.uniform(bounds[0], bounds[1], (num_particles, 2))\n",
    "    personal_best_positions = particles.copy()\n",
    "    personal_best_scores = np.array([evaluate_mlp_model(p[0], p[1]) for p in particles])\n",
    "    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "    global_best_score = np.min(personal_best_scores)\n",
    "\n",
    "    convergence = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        for i, particle in enumerate(particles):\n",
    "            velocity = np.random.uniform(-1, 1, 2)\n",
    "            new_position = particle + velocity\n",
    "            new_position = np.clip(new_position, bounds[0], bounds[1])\n",
    "            particles[i] = new_position\n",
    "\n",
    "            score = evaluate_mlp_model(new_position[0], new_position[1])\n",
    "            if score < personal_best_scores[i]:\n",
    "                personal_best_positions[i] = new_position\n",
    "                personal_best_scores[i] = score\n",
    "\n",
    "                if score < global_best_score:\n",
    "                    global_best_position = new_position\n",
    "                    global_best_score = score\n",
    "\n",
    "        convergence.append(global_best_score)\n",
    "\n",
    "    return global_best_position, -global_best_score, convergence\n",
    "\n",
    "bounds = [(32, 256), (16, 128)]\n",
    "\n",
    "best_hyperparameters, best_accuracy, convergence = pso_optimization_mlp(bounds, num_particles=10, num_iterations=100)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hyperparameters)\n",
    "print(\"Best accuracy found:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "search_space = {\n",
    "    'num_units': [32, 64, 128],\n",
    "    'num_layers': range(1, 4)\n",
    "}\n",
    "\n",
    "population_size = 10\n",
    "num_iterations = 100\n",
    "c1 = 2.0\n",
    "c2 = 2.0\n",
    "w = 0.7\n",
    "\n",
    "def create_rnn_model(params, input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.SimpleRNN(params['num_units'], activation='relu', input_shape=input_shape))\n",
    "\n",
    "    for _ in range(params['num_layers']):\n",
    "        model.add(layers.Dense(params['num_units'], activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_rnn_model(params, input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    model = create_rnn_model(params, input_shape, num_classes)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x=X_train, y=y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def pso(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    population = initialize_population(population_size, search_space)\n",
    "    velocity = [{param: 0 for param in search_space.keys()} for _ in range(population_size)]\n",
    "    personal_best = population.copy()\n",
    "    global_best = None\n",
    "    global_best_fitness = -1\n",
    "\n",
    "    convergence = []\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
    "        for i, individual in enumerate(population):\n",
    "            fitness = evaluate_rnn_model(individual, input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "            if fitness > evaluate_rnn_model(personal_best[i], input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "                personal_best[i] = individual.copy()\n",
    "            if fitness > global_best_fitness:\n",
    "                global_best = individual.copy()\n",
    "                global_best_fitness = fitness\n",
    "\n",
    "            for param in search_space.keys():\n",
    "                velocity[i][param] = w * velocity[i][param] + c1 * random.random() * (personal_best[i][param] - individual[param]) + c2 * random.random() * (global_best[param] - individual[param])\n",
    "                individual[param] = int(round(individual[param] + velocity[i][param]))\n",
    "                individual[param] = max(min(individual[param], max(search_space[param])), min(search_space[param]))\n",
    "\n",
    "        convergence.append(global_best_fitness)\n",
    "\n",
    "    return global_best, global_best_fitness, convergence\n",
    "\n",
    "def initialize_population(population_size, search_space):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        individual = {param: random.choice(values) for param, values in search_space.items()}\n",
    "        population.append(individual)\n",
    "    return population\n",
    "\n",
    "input_shape = (22, 4)\n",
    "num_classes = 4\n",
    "split_index = int(0.8 * X.shape[0])\n",
    "X_train, X_test = tf.split(X, [split_index, X.shape[0] - split_index])\n",
    "y_train, y_test = tf.split(y, [split_index, y.shape[0] - split_index])\n",
    "\n",
    "best_individual, best_fitness = pso(population_size, num_iterations, search_space, input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "print(\"Best RNN Model Architecture found:\")\n",
    "print(best_individual)\n",
    "print(\"Best Accuracy:\", \"%.3f\" %best_fitness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
